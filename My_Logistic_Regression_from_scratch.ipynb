{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN46B8YCakoAaboKEUcuOGx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajshree-Th/Logistic-regression/blob/main/My_Logistic_Regression_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3hTmSIpvHL"
      },
      "source": [
        "# Implement Logistic Regression from scratch\n",
        "\n",
        "In this session, we will implement Logistic Regression with L1 regularization from scratch and predict the labels of the test data. We will then verify the correctness of the our implementation using multiple \"grader\" functions/cells. The grader functions would help us to validate the correctness of our code.\n",
        "\n",
        "Let's know a brief about Logistic regression. Logistic regression is a statistical method used to model the relationship between a binary dependent variable (a variable that can take only two values, such as \"yes\" or \"no\") and one or more independent variables. The goal of logistic regression is to predict the probability of the dependent variable taking one of the two possible values based on the values of the independent variables.\n",
        "\n",
        "It is a popular and powerful method for modeling the probability of binary outcomes based on one or more independent variables. It is widely used in various fields such as finance, healthcare, marketing, and social sciences.\n",
        "\n",
        "The foremost task is to get data. I already have the CSV data file is in my google drive. So to directly get access our data file stored in google drive into our google colab notebook we need to mount Google Drive. Once mounted, the we can navigate to the \"/gdrive\" directory in the Colab notebook and access the files stored in Google Drive as if they were stored locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7BftW1ZeJzq",
        "outputId": "ea3feea9-b55e-482a-e457-94dd263ea3f7"
      },
      "source": [
        "# Mounting google drive as we are loading the data from google drive\n",
        "# Importing \"drive\" from \"google.colab\"\n",
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've successfully mounted google drive into our google colab notebook, we can now read the CSV file into a pandas DataFrame by importing pandas in Google Colab. "
      ],
      "metadata": {
        "id": "boLJWCJMxo-C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "uOue_wDrelWm",
        "outputId": "11c14788-e90b-4bb9-cf77-d4b092f68755"
      },
      "source": [
        "# Importing pandas library and assigns it an alias \"pd\"\n",
        "import pandas as pd\n",
        "\n",
        "# Specifying the path of the CSV file that is located in the Google Drive\n",
        "data_path = \"/gdrive/My Drive/My DB/logistic_regression_assignment_data.csv\"\n",
        "\n",
        "# Reading the CSV file into a pandas DataFrame and assigns it to a variable 'df'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Displaying the contents of the DataFrame \"df\"\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                               text\n",
              "0            0  worldcom boss  left books alone  former worldc...\n",
              "1            1  tigers wary of farrell  gamble  leicester say ...\n",
              "2            1  yeading face newcastle in fa cup premiership s...\n",
              "3            1  henman hopes ended in dubai third seed tim hen...\n",
              "4            1  wilkinson fit to face edinburgh england captai...\n",
              "...        ...                                                ...\n",
              "1012         0  wall street cool to ebay s profit shares in on...\n",
              "1013         0  ban on forced retirement under 65 employers wi...\n",
              "1014         1  time to get tough on friendlies  for an intern...\n",
              "1015         0  christmas shoppers flock to tills shops all ov...\n",
              "1016         0  bush budget seeks deep cutbacks president bush...\n",
              "\n",
              "[1017 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1185745a-bafd-4d45-8d5e-1570a1a83b22\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1012</th>\n",
              "      <td>0</td>\n",
              "      <td>wall street cool to ebay s profit shares in on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1013</th>\n",
              "      <td>0</td>\n",
              "      <td>ban on forced retirement under 65 employers wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>1</td>\n",
              "      <td>time to get tough on friendlies  for an intern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas shoppers flock to tills shops all ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>0</td>\n",
              "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1017 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1185745a-bafd-4d45-8d5e-1570a1a83b22')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1185745a-bafd-4d45-8d5e-1570a1a83b22 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1185745a-bafd-4d45-8d5e-1570a1a83b22');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtSIu_HSp5as"
      },
      "source": [
        "We can see that \"df\" contains 1016 rows and 2 columns- \"text\" and \"category\". The 'category' column is a binary variable that indicates the category of news article: 0 if the article is business-related and 1 if it is sports-related. The 'text' column contains the text content of the news article.\n",
        "\n",
        "Looking at the count of the number of occurrences of each unique value in the 'category' column, there are 508 sport-related entries and 509 business-related entries. Hence this dataset is considered to be nearly balanced. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpzwECIbfpDi",
        "outputId": "6bf96dbd-8050-4776-d3f6-5485290a15f4"
      },
      "source": [
        "df[\"category\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    509\n",
              "0    508\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXn39LKsp_L4"
      },
      "source": [
        "### Creating Train and Test Datasets\n",
        "\n",
        "Creating training and test data is an important step in Machine learning. This allows us to evaluate the model's performance on unseen data to ensure that it can generalize well to new, unseen data points.\n",
        "\n",
        "So to do this we've imported \"train_test_split\" function from the scikit-learn library which is used to split a dataset into training and testing sets. This function shuffles the data randomly before splitting it into training and testing sets, which helps to ensure that the distribution of samples in each set is representative of the overall dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj4XnEORk3aY",
        "outputId": "54689991-08ad-4542-8390-408f349242ef"
      },
      "source": [
        "# Importing train_test_split from Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assigning the columns \"text\" and \"category\" into variables \"text\" and \"category\" \n",
        "text, category = df['text'], df['category']\n",
        "'''\n",
        "test_size=0.01, this means that 1% of the data will be used for testing and 99% for training. \n",
        "\"random_state\" parameter is set to 42 to ensure that the data is split in the same way every time the function is run \n",
        "\"stratify\" parameter is set to \"category\" to ensure that the proportion of sports-related and business-related articles \n",
        "is roughly the same in both the training and testing sets so that the model is not biased towards one class.\n",
        "'''\n",
        "train_text, test_text, train_category, test_category = train_test_split(text, category, test_size = 0.01, random_state = 42, stratify = category)\n",
        "\n",
        "# Displaying the shapes of train and test data\n",
        "print(\"Shape of Train_Text = \", train_text.shape)\n",
        "print(\"Shape of Test_Text = \", test_text.shape)\n",
        "print(\"Shape of Train_Category = \", train_category.shape)\n",
        "print(\"Shape of Train_Category = \", test_category.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Train_Text =  (1006,)\n",
            "Shape of Test_Text =  (11,)\n",
            "Shape of Train_Category =  (1006,)\n",
            "Shape of Train_Category =  (11,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fCMr9kAqcWG"
      },
      "source": [
        "### Procedure:\n",
        "\n",
        "  1. Reading the train_data.\n",
        "  2. Vectorizing train_data and test_data using sklearns built in tfidf vectorizer.\n",
        "  3. Ignoring unigrams and make use of both **bigrams & trigrams** and also limit the **max features** to **2000** and **minimum document frequency** to **10**.\n",
        "  4. After the tfidf vectors are generated as mentioned above, next task is to column standardize the data.\n",
        "  5. To write the reason for standardizing the data.\n",
        "  6. Using sklearn StandardScaler to column standardize the data.\n",
        "  7. To write a function to initialise weights & bias. And then to run its corresponding grader function.\n",
        "  8. To write a custom function to calculate sigmoid of a value. And then to run its corresponding grader function to cross check our implementation of sigmoid function.\n",
        "  9. To write a custom function to compute the total loss as the sum of log loss and l1 regularization loss based on true labels and predicted labels and weights. And then to crosscheck our implementation with its corresponding grader.\n",
        "  10. To write a function to compute gradients for our weights and bias terms, which we have to make use of in updating our weights and bias while training our model.\n",
        "  11. To implement a custom train function of logistic regression, wherein we take in the following inputs:\n",
        "        * **X_train** which will be our vectorized text data\n",
        "        * **y_train** which are the labels for our train data\n",
        "        * **alpha** = 0.0001 which is the regularization factor (λ) \n",
        "        * **eta0** = 0.0001 which will be the learning rate   \n",
        "        * **tolerance** = 0.001\n",
        "  12. In the custom train function we should make use of a custom SGD function to update the weights and bias terms for **each** of your inputs. \n",
        "  13. The custom SGD implemented in the above train function for updating the weights and bias terms should run for many epochs until the difference in loss between two consecutive epochs is less than tolerance.\n",
        "  14. Here one epoch means a complete iteration of our entire train data.\n",
        "  15. Our train function should return the following:\n",
        "        * the number of epochs it took to complete the training.\n",
        "        * train loss for all epochs.\n",
        "        * the values for final weights and bias terms.  \n",
        "  16. Now we run the grader function to check whether the weights and bias obtained from our custom implementation are close enough to that of sklearns implementation.\n",
        "  17. Next we write a custom predict function which takes in as input the weights and bias values that was computed in train function, and also takes in the test standardized data as input to predict its labels.\n",
        "  18. Now we run the grader function to check the accuracy of our predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5NqSfQ7qcNT"
      },
      "source": [
        "###Import necessary libraries\n",
        "\n",
        "Importing some of the libraries that will be used during this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swevXoO-1V1G"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6in6_LL6qqcB"
      },
      "source": [
        "### 1. Vectorize train data and test data using sklearn tf-idf\n",
        "\n",
        "\n",
        "TfidfVectorizer is a valuable tool in natural language processing and machine learning. It stands for Term Frequency-Inverse Document Frequency, which is a statistical measure that reflects how important a word is to a document in a collection or corpus. It helps in text classification and clustering by transforming text into a numerical representation, improving the efficiency and effectiveness of algorithms. It can also aid in text preprocessing by removing stop words, stemming, and converting text to lowercase.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3byFeuL2gX4",
        "outputId": "f04a9d82-3532-481d-fb40-a94d6dee6414"
      },
      "source": [
        "'''\n",
        "\"ngram_range\" parameter is set to (2,3), which means that the vectorizer will extract both 2-grams (pairs of adjacent words) and \n",
        "3-grams (triplets of adjacent words) from the text. This can help capture more complex relationships between words and improve the model's performance.\n",
        "\"max_features\" parameter is set to 2000, which specifies the maximum number of features.\n",
        "This can help to reduce the dimensionality of the data and prevent overfitting.\n",
        "\"min_df\" parameter is set to 10, which means that the vectorizer will exclude any words or n-grams that appear in fewer than 10 documents. \n",
        "This can help to remove rare and potentially irrelevant features from the data and improve the model's performance.\n",
        "'''\n",
        "vectorizer = TfidfVectorizer(ngram_range = (2,3), max_features = 2000, min_df = 10)\n",
        "'''\n",
        "\"fit_transform()\" is used to learn and transform the training data, \n",
        "while \"transform()\" is used to apply the learned transformation to the test or new data.\n",
        "'''\n",
        "# Vectorizing train and test data using TF-IDF and store them in train_vectors and test_vectors respectively\n",
        "train_vectors = vectorizer.fit_transform(train_text)\n",
        "test_vectors = vectorizer.transform(test_text)\n",
        "\n",
        "# Displaying shapes of train and test vectors\n",
        "train_vectors.shape, test_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1006, 2000), (11, 2000))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqhohjjQqw7v"
      },
      "source": [
        "###2. Column standardize the train and test data\n",
        "\n",
        "Column standardization is used in machine learning to transform the data so that each feature has equal importance in the model. It ensures that features with different scales or ranges don't dominate the others and cause biased results. It also improves the model's robustness to outliers and helps it converge faster during training. \n",
        "\n",
        "Standardizing the data enhances the performance of some machine learning algorithms and improves the model's ability to generalize to new data.\n",
        "\n",
        "Assuming the data is Guassian distributed, in order overcome underfitting, column standardization is done to squish the data such that the mean comes at origin with variance of one (mu=0, sigma=1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBwydrVH4zve",
        "outputId": "0997b5b2-6532-4c5b-822d-d0dcac4286f0"
      },
      "source": [
        "# Creating an instance of \"StandardScaler()\" class\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Column standardizing the train and test data and store them in train_vectors_stand and test_vectors_stand\n",
        "train_vectors_stand = scaler.fit_transform(np.asarray(train_vectors.todense())) \n",
        "test_vectors_stand = scaler.transform(np.asarray(test_vectors.todense()))\n",
        "\n",
        "# Displaying shapes of standardised train and test vectors after assigning them to \"n_samples\" and \"d_features\"\n",
        "train_vectors_stand.shape, test_vectors_stand.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1006, 2000), (11, 2000))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the number of rows and columns of training data to separate variables\n",
        "n_samples, d_features = train_vectors_stand.shape\n",
        "n_samples, d_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4LK5q9ae2yt",
        "outputId": "d4978b3b-0534-4496-abc5-63deed6a41ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1006, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUn89OSMrFRi"
      },
      "source": [
        "### 3. Initialize weights and bias terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdxfHIyAEJ-D"
      },
      "source": [
        "def initialize_weights_bias(dim):\n",
        "    ''' In this function, we will initialize our weights and bias terms'''\n",
        "    # Weights is an array of (dim) dimensions. \n",
        "    # Here dim is the number of features of tfidf vectorizer output.\n",
        "    w = np.zeros((dim))\n",
        "    b = 0\n",
        "    return w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elTV2z8xrUKQ"
      },
      "source": [
        "### Grader Function - 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsAL2cKVfPWB",
        "outputId": "4226128f-5777-4fea-a403-17f15638dbea"
      },
      "source": [
        "# Grader function-1 is design to check the initialization of your weights and bias terms.\n",
        "def grader_weights_bias(w,b):\n",
        "    assert((len(w)==2000) and b==0)\n",
        "    return True\n",
        "\n",
        "dim = 2000\n",
        "w,b = initialize_weights_bias(dim)\n",
        "grader_1 = grader_weights_bias(w,b)\n",
        "print(\"Grader_1 Status : \", grader_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_1 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE87BSjfrZPo"
      },
      "source": [
        "### 4. Calculate sigmoid of a value\n",
        "\n",
        "The sigmoid function is used in logistic regression because it maps any input value to a value between 0 and 1, which can be interpreted as a probability. The sigmoid function has a characteristic \"S\" shape, which makes it easy to interpret the output of the model as a probability.\n",
        "\n",
        "Mathematically, the sigmoid function is defined as:\n",
        "\n",
        "σ(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "where z is the weighted sum of the input features and σ(z) is the output probability. The sigmoid function is differentiable, which makes it suitable for use in optimization algorithms such as gradient descent. The derivative of the sigmoid function is also relatively simple, which makes it easy to calculate gradients for backpropagation during training.\n",
        "\n",
        "Overall, the sigmoid function is a natural choice for logistic regression because of its ability to map inputs to probabilities and its mathematical properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDZlQwcKj3K0"
      },
      "source": [
        "def custom_sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # Computing sigmoid(z) and returning its value.\n",
        "    sigmoid = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return sigmoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRINF_o4rc8Z"
      },
      "source": [
        "### Grader Function - 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXGvnegSj_GV",
        "outputId": "1b41f71e-b64c-4065-c96f-4987c233f282"
      },
      "source": [
        "# Grader function to check the implementaiton of sigmoid function\n",
        "def grader_sigmoid(z):\n",
        "    val = custom_sigmoid(z)\n",
        "    assert(val==0.8807970779778823)\n",
        "    return True\n",
        "\n",
        "grader_2 = grader_sigmoid(2)\n",
        "print(\"Grader_2 Status : \", grader_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_2 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw5SpoGkriy9"
      },
      "source": [
        "### 5.  Compute loss function\n",
        "\n",
        "Let's now calculate loss function with L1 regularisation\n",
        "\n",
        "$logloss = -1*\\frac{1}{n}\\Sigma_{for each Y_{true},Y_{pred}}(Y_{true}log10(Y_{pred})+(1-Y_{true})log10(1-Y_{pred}))$\n",
        "\n",
        "$L1 loss = \\Sigma_{for each w}(|w|)$\n",
        "\n",
        "$total loss = logloss + alpha*L1loss$<br>\n",
        "where alphas is the regularization parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPichdD6l4AP"
      },
      "source": [
        "def custom_loss(y_true, y_pred, alpha, w):\n",
        "    '''In this function, we will compute total loss which is \n",
        "    [(logloss) + (alpha * L1regularization loss)] '''\n",
        "    \n",
        "    # Initializing variables with values zeroes.\n",
        "    temp1, temp2 = 0, 0\n",
        "    \n",
        "    for i in range(0,len(y_true)):\n",
        "        if y_true[i] == 0:\n",
        "            temp1 += np.log10(1 - y_pred[i])\n",
        "        else:\n",
        "            temp1 += np.log10(y_pred[i])   \n",
        "    log_loss = (-1) * temp1/len(y_true)\n",
        "  \n",
        "    for j in w:\n",
        "        temp2 += abs(j)\n",
        "        l1_loss = temp2\n",
        "\n",
        "    total_loss = log_loss + alpha * l1_loss \n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToztdaH9rzWv"
      },
      "source": [
        "### Grader Function - 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBB39zGxmLmb",
        "outputId": "48a899e4-a23f-4a03-8c05-26469400aabe"
      },
      "source": [
        "# Grader function to check the implementaiton of logloss\n",
        "def grader_loss():\n",
        "    true_values = [1,1,0,1,0]\n",
        "    pred_values = [0.9,0.8,0.1,0.8,0.2]\n",
        "    w= np.array([0.1]*10)\n",
        "    alpha= 0.0001\n",
        "    loss = custom_loss(true_values, pred_values,alpha,w)\n",
        "    assert(loss==0.07644900402910389+0.0001*10*0.1)\n",
        "    return True\n",
        "\n",
        "\n",
        "grader_3 = grader_loss()\n",
        "print(\"Grader_3 Status : \", grader_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_3 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnw2qtkUr8Ha"
      },
      "source": [
        "### 6. Function to updated weights and bias terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0sM2AUOr_Fz"
      },
      "source": [
        "Uing the formula below to compute gradient of our weight and bias terms <br>\n",
        "Loss term Li for a single example is given as below: \n",
        "<br>\n",
        "$Li= -(Y_{i}log10(𝝈_{i})-(1-Y_{i})log10(1-𝝈_{i}) + \\frac{alpha}{N}(sum(|w|))\n",
        "$ <br>\n",
        "\n",
        "$Where: 𝝈_{i} = σ(w^{T} x_i+b) $ <br>\n",
        "<br>\n",
        "And: \n",
        "\n",
        "L1 regularization = $\\frac{alpha}{N}(sum(|w|)) $ <br>\n",
        "Alpha: Regularization parameter <br>\n",
        "N : number of training examples<br>\n",
        "σ : sigmoid function <br>\n",
        "<br>\n",
        "$dLi/dw= -Y_{i}x_{i}(1-𝝈_{i}) + (1-Y_{i})x_{i}𝝈_{i} + \\frac{alpha}{N} \\frac{w + (1e-5)}{|w + (1e-5)|}  $<br>\n",
        "NOTE THAT: 1e-5 is used in numerator and denominator to avoid division error <br>\n",
        "\n",
        "$dLi/db= -Y_{i}(1-𝝈_{i}) + (1-Y_{i})𝝈_{i}$<br>\n",
        "<br>\n",
        "Hence,<br>\n",
        "$dLi/dw= dw = (𝝈_{i} -Y_{i})x_{i} + \\frac{alpha}{N}\\frac{w + (1e-5)}{|w + (1e-5)|} $<br>\n",
        "1e-5 is used in numerator and denominator to avoid division error <br>\n",
        "$dLi/db =  db = 𝝈_{i}-Y_{i}$\n",
        "<br>\n",
        "\n",
        "!!NOTE: NEGATIVE GRADIENT IS USED WHILE UPDATING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAzY9c1MsFu3"
      },
      "source": [
        "### 6a. Compute Gradient of loss function wrt weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R41awnBzuemk"
      },
      "source": [
        "def gradient_dw(x, y, w, b, alpha, N):\n",
        "    '''In this function, we will compute the gardient w.r.t. w '''\n",
        "    y_hat = custom_sigmoid(np.dot(w,x.T) + b)\n",
        "    dw = np.dot((y_hat - y),x) + ((alpha/N)*((w+1e-5)/abs(w+1e-5)))\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3RoeRPzsNKe"
      },
      "source": [
        "### 6b.  Custom function to compute Gradient of loss function wrt bias term:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBX-WiTvMSJ"
      },
      "source": [
        "def gradient_db(x, y, w, b):\n",
        "    '''In this function, we will compute the gardient w.r.t. b '''\n",
        "    y_hat = custom_sigmoid(np.dot(w,x.T) + b)\n",
        "    db = y_hat - y\n",
        "    return db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMG3OTjJsWJz"
      },
      "source": [
        "###6c. Custom function to train logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haWXUqXbsZ5T"
      },
      "source": [
        "$w^{(t+1)}← w^{(t)}- eta0*(dw^{(t)}) $<br>\n",
        "$b^{(t+1)}←b^{(t)} - eta0*(db^{(t)}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af0IoUfUeDPJ"
      },
      "source": [
        "def custom_train(X_train, y_train, alpha, eta0, tolerance):\n",
        "    \"\"\"In this function we will compute optimal values for weights and bias terms on the train data. \n",
        "    Here eta0 is the learning rate and alpha is the regularization term.\"\"\"\n",
        "    \n",
        "    train_loss=[]\n",
        " \n",
        "    # Initializing weights and bias by calling the initializing function\n",
        "    w, b = initialize_weights_bias(d_features)\n",
        "    # Initializing a variable to count the number of iterations. It is initially set to zero\n",
        "    num_epochs = 0\n",
        "    # Assigning a boolean value \"True\" to variable \"condition\". This is done to start the \"while\" loop\n",
        "    condition = True\n",
        "\n",
        "    while(condition):\n",
        "        # Iterating every data points  \n",
        "        for n in range(n_samples):\n",
        "\n",
        "            # Computing \"dw\" and \"db\" by calling their respective gradient functions \n",
        "            dw = gradient_dw(X_train[n], y_train[n], w, b, alpha, n_samples)\n",
        "            db = gradient_db(X_train[n], y_train[n], w, b)\n",
        "            # Updating \"w\" and \"b\"\n",
        "            w = w - eta0 * dw\n",
        "            b = b - eta0 * db\n",
        "\n",
        "        # Predicting the output of train data\n",
        "        y_pred = []    \n",
        "        for point in X_train:\n",
        "            y_pred = np.append(y_pred, (custom_sigmoid(np.dot(w,point.T)+b)))\n",
        "        # Computing loss (error) between the predicted and actual values and storing them into  \n",
        "        train_loss.append(custom_loss(y_train, y_pred, alpha, w))\n",
        "        \n",
        "        # Checking if the difference of two consecutive losses exceeds the tolerance value, otherwise stop the loop \n",
        "        if num_epochs > 0:\n",
        "            diff = train_loss[num_epochs-1] - train_loss[num_epochs]\n",
        "            condition = (diff >= tolerance)\n",
        "        num_epochs = num_epochs + 1\n",
        "\n",
        "    # Return the values of weights, bias, train_loss and num_epochs\n",
        "    return w,b,train_loss, num_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eULpgHP2rdNr",
        "outputId": "e64d6e5f-c839-4eb8-a31b-918b546585c6"
      },
      "source": [
        "# Calling the above \"custom_train\" function and displaying the outputs\n",
        "w,b,_,epoch = custom_train(train_vectors_stand, train_category.values, 0.0001, 0.0001, 0.001)\n",
        "print(\"W :\", w)\n",
        "print(\"B :\",  b)\n",
        "print(\"Train loss :\", _)\n",
        "print(\"Num_Epochs :\", epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W : [-0.02309065 -0.01788942  0.01964919 ...  0.0178764   0.02193327\n",
            "  0.01974338]\n",
            "B : 0.003429151632644843\n",
            "Train loss : [0.16110780094782637, 0.10963558894017825, 0.08397834542265481, 0.06865241727222729, 0.05843450877922999, 0.0511112817434264, 0.045589658035246665, 0.041268077415098345, 0.037787821996590615, 0.03492147944756584, 0.03251757279289221, 0.030471172388287938, 0.02870724638945493, 0.02717072229086866, 0.02582011906943329, 0.02462351883196356, 0.02355596022164322, 0.022597652954307084]\n",
            "Num_Epochs : 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhJpKQfsisz"
      },
      "source": [
        "### Grader Function - 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Xk4q8w3Ned",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84d1233-fd54-4172-e364-847f43cd9bda"
      },
      "source": [
        "def grader_weights_bias():\n",
        "    # Fitting sklearn SGD classifier\n",
        "    clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log_loss', \n",
        "                                     random_state=15, penalty='l1', tol=1e-3, learning_rate='constant')\n",
        "    clf.fit(train_vectors_stand, train_category.values)\n",
        "    model_coef= clf.coef_[0]\n",
        "\n",
        "    # Fitting custom train with same learning rate, regularization and tolerance as of sklearn\n",
        "    w,b,_,epoch = custom_train(train_vectors_stand, train_category.values, 0.0001, 0.0001, 0.001)\n",
        "\n",
        "    # Checking whether the weights and bias returned by both the implementations are closer\n",
        "    assert((not (w-model_coef>0.02).any())==True)\n",
        "    assert(not (b-clf.intercept_>0.02)==True)\n",
        "  \n",
        "    return True\n",
        "\n",
        "grader_4 = grader_weights_bias()\n",
        "print(\"Grader_4 Status : \", grader_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grader_4 Status :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzb9pmbZsqMK"
      },
      "source": [
        "### 7. Plot the train loss with x as epoch number and y as train loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gxFCu4sB4xL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "c7bb31c7-0c7e-4bce-af9e-13131c4d4cc7"
      },
      "source": [
        "'''Plotting graph for epoch vs loss for train and test data by calling \"custom_train\" function'''\n",
        "w,b,train_loss,epochs = custom_train(train_vectors_stand, train_category.values, 0.0001, 0.0001, 0.001)\n",
        "plt.plot(range(epochs), train_loss, label='train curve')\n",
        "plt.title('epoch vs loss')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEYCSRfIsuNx"
      },
      "source": [
        "### 8. Custom function to make predictions using logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7PVyRIjknvN"
      },
      "source": [
        "def predict(w,b, X):\n",
        "    '''function to predict label given weights, bias and standardized data'''\n",
        "    predictions = []\n",
        "    threshold = 0.5\n",
        "    \n",
        "    # Iterating each point of X\n",
        "    for point in X:\n",
        "        # Predicting each point by calling \"custom_sigmoid\" function \n",
        "        preds = custom_sigmoid(np.dot(w,point.T) + b)\n",
        "        \n",
        "        # Checking whether the prediction is greater than or less than the threshold \n",
        "        if  preds < threshold:\n",
        "            # Appending prediction array with \"0\" if the prediction is less than the threshold\n",
        "            predictions = np.append(predictions,0)\n",
        "        else:\n",
        "            # Assigning prediction array with \"1\" if the prediction is greater than the threshold\n",
        "            predictions = np.append(predictions,1)\n",
        "\n",
        "    #print(predictions)\n",
        "    return predictions # Numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfWCR8mIs0rq"
      },
      "source": [
        "### Grader Function - 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ9qoiEJlCQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac773eaa-13c5-4b73-87b9-c2cdf2835a30"
      },
      "source": [
        "def grader_predict():\n",
        "  ''' grader to check the test accuracy'''\n",
        "  w, b, _, _ = custom_train(train_vectors_stand, train_category.values, 0.0001, 0.0001, 0.001)\n",
        "  test_preds= predict(w, b, test_vectors_stand)\n",
        "  test_accuracy= (np.sum(test_category==test_preds) / len(test_preds)) * 100\n",
        "  if(test_accuracy >= 90):\n",
        "    print(\"Success!\")\n",
        "  else:\n",
        "    print(\"Failed! \\n Test accuracy = \", test_accuracy)\n",
        "  return\n",
        "  \n",
        "grader_predict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    }
  ]
}